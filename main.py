from dotenv import load_dotenv
import os
import json
from tempfile import SpooledTemporaryFile
from typing import List, Optional

import openai
from fastapi import FastAPI, UploadFile, File, Form, Depends
from pydantic import BaseModel
try:
    import docx
except Exception:  # pragma: no cover - optional dependency
    docx = None
try:
    import fitz  # PyMuPDF
except Exception:  # pragma: no cover - optional dependency
    fitz = None
from utils.simple_split import classify_text

# ------------------------------------------------------------------
#  authentication helper
# ------------------------------------------------------------------
from auth import require_api_key  # -> auth.py contains require_api_key()

# ------------------------------------------------------------------
#  load environment / set OpenAI
# ------------------------------------------------------------------
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
print("üîë API KEY:", "FOUND" if openai_api_key else "NOT FOUND")
client = openai.OpenAI(api_key=openai_api_key)

# ------------------------------------------------------------------
#  FastAPI app
# ------------------------------------------------------------------
app = FastAPI(
    title="LawQB Immigration Legal AI API",
    description="Upload declarations and receive legal analysis and motion drafting assistance. Powered by GPT‚Äë4.",
    version="1.0.0",
)


@app.get("/health")
def healthcheck():
    return {"status": "ok"}


# ------------------------------------------------------------------
#  /analyze
# ------------------------------------------------------------------
class AnalyzeRequest(BaseModel):
    question: str
    jurisdiction: Optional[str] = None
    preferredSources: Optional[List[str]] = None


class AnalyzeResponse(BaseModel):
    issue: str
    rule: str
    application: str
    conclusion: str
    citations: List[str]
    conflictsOrAmbiguities: str
    verificationNotes: str


@app.post(
    "/analyze",
    response_model=AnalyzeResponse,
    dependencies=[Depends(require_api_key)],
)
def analyze(req: AnalyzeRequest):
    prompt = f"""
You are an expert U.S. immigration attorney. Use the IRAC format to answer this legal question.

Question: {req.question}
Jurisdiction: {req.jurisdiction or "General U.S. immigration law"}

Respond in raw JSON only (no markdown), with the following fields:
- issue
- rule
- application
- conclusion
- citations (list of strings)
- conflictsOrAmbiguities
- verificationNotes
"""
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {
                    "role": "system",
                    "content": "You are an immigration law expert. Reply using only IRAC format in JSON.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )
        content = response.choices[0].message.content.strip()
        if content.startswith("```json"):
            content = content.replace("```json", "").strip()
        if content.endswith("```"):
            content = content[:-3].strip()

        print("GPT IRAC Output:", content)
        parsed = json.loads(content)
        return AnalyzeResponse(**parsed)

    except Exception as e:
        print(f"‚ùå Analyze endpoint error: {e}")
        return AnalyzeResponse(
            issue="Error generating analysis",
            rule="",
            application="",
            conclusion="",
            citations=[],
            conflictsOrAmbiguities="",
            verificationNotes=f"Exception: {str(e)}",
        )


# ------------------------------------------------------------------
#  /uploadEvidence
# ------------------------------------------------------------------
class SummarizeEvidenceResponse(BaseModel):
    filename: str
    sizeInBytes: int
    readableSize: str
    fileType: str
    truncated: bool
    category: str
    summary: str
    keyFacts: List[str]
    legalIssues: List[str]
    credibilityConcerns: str
    recommendation: str
    verificationNotes: str


@app.post(
    "/uploadEvidence",
    response_model=SummarizeEvidenceResponse,
    dependencies=[Depends(require_api_key)],
)
async def upload_evidence(
    file: UploadFile = File(...),
    jurisdiction: Optional[str] = Form(None),
    context: Optional[str] = Form("Asylum"),
):
    ext = file.filename.lower().split(".")[-1]
    content = ""
    truncated = False
    total_bytes = 0
    readable_size = "Unknown"

    try:
        temp_file = SpooledTemporaryFile(max_size=1024 * 1024 * 100)
        while chunk := await file.read(1024 * 1024):
            total_bytes += len(chunk)
            temp_file.write(chunk)
        temp_file.seek(0)
        readable_size = f"{round(total_bytes / 1024, 1)} KB"

        page_count = 0

        # ---------- DOCX ----------
        if ext == "docx":
            if not docx:
                raise ValueError("docx support not available")
            document = docx.Document(temp_file)
            content = "\n".join(
                [p.text for p in document.paragraphs if p.text.strip()]
            )

        # ---------- PDF (with OCR fallback) ----------
        elif ext == "pdf":
            if not fitz:
                raise ValueError("pdf support not available")
            pdf_bytes = temp_file.read()

            # 1) embedded text
            pdf = fitz.open(stream=pdf_bytes, filetype="pdf")
            page_count = len(pdf)
            content = "".join(page.get_text() or "" for page in pdf)
            pdf.close()

            # 2) OCR if empty
            if not content.strip():
                try:
                    from pdf2image import convert_from_bytes
                    import pytesseract
                    import io

                    images = convert_from_bytes(pdf_bytes, fmt="png")
                    ocr_parts = []
                    for img in images:
                        buf = io.BytesIO()
                        img.save(buf, format="PNG")
                        ocr_parts.append(pytesseract.image_to_string(buf.getvalue()))
                    content = "\n".join(ocr_parts)
                except Exception:
                    content = ""

        # ---------- TXT ----------
        elif ext == "txt":
            content = temp_file.read().decode("utf-8")

        else:
            raise ValueError("Unsupported file type")

        category = classify_text(content, size_bytes=total_bytes, num_pages=page_count)

    except Exception as e:
        return SummarizeEvidenceResponse(
            filename=file.filename,
            sizeInBytes=total_bytes,
            readableSize=readable_size,
            fileType=ext,
            truncated=False,
            category="unknown",
            summary="Could not process file.",
            keyFacts=[],
            legalIssues=[],
            credibilityConcerns="",
            recommendation="",
            verificationNotes=f"File processing error: {str(e)}",
        )

    # ---------- Chunking ----------
    MAX_CHARS = 11_000
    chunks = [content[i : i + MAX_CHARS] for i in range(0, len(content), MAX_CHARS)]
    if len(chunks) > 1:
        truncated = True

    # ---------- GPT helper ----------
    def gpt_analyze(text_chunk: str):
        prompt = f"""
You are an expert immigration attorney analyzing part of a legal evidence document.

Jurisdiction: {jurisdiction or "General U.S. immigration law"}
Context: {context or "Asylum"}

Summarize the text, extract key facts, identify legal issues, note credibility concerns, and give a legal recommendation.

Text:
{text_chunk}

Return JSON only:
- summary
- keyFacts (list of strings)
- legalIssues (list of strings)
- credibilityConcerns
- recommendation
- verificationNotes
"""
        try:
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {
                        "role": "system",
                        "content": "You are a senior immigration attorney. Reply ONLY in flat JSON. No markdown.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.3,
            )
            result = response.choices[0].message.content.strip()
            if result.startswith("```json"):
                result = result.replace("```json", "").strip()
            if result.endswith("```"):
                result = result[:-3].strip()

            # debug print
            print("\n--- GPT RAW RESPONSE START ---")
            print(result)
            print("--- GPT RAW RESPONSE END ---\n")

            return json.loads(result)

        except Exception as e:
            print(f"‚ùå GPT error during evidence chunk analysis: {e}")
            return {
                "summary": "Error during GPT analysis.",
                "keyFacts": [],
                "legalIssues": [],
                "credibilityConcerns": "",
                "recommendation": "",
                "verificationNotes": f"GPT error: {e}",
            }

    # ---------- Aggregate ----------
    summaries: List[str] = []
    keyFacts: List[str] = []
    legalIssues: List[str] = []
    credibilityNotes: List[str] = []
    recommendations: List[str] = []
    verificationNotes: List[str] = []

    for chunk in chunks:
        parsed = gpt_analyze(chunk)
        summaries.append(parsed.get("summary", ""))
        keyFacts.extend(parsed.get("keyFacts", []))
        legalIssues.extend(parsed.get("legalIssues", []))
        credibilityNotes.append(parsed.get("credibilityConcerns") or "")
        recommendations.append(parsed.get("recommendation") or "")
        verificationNotes.append(parsed.get("verificationNotes") or "")

    return SummarizeEvidenceResponse(
        filename=file.filename,
        sizeInBytes=total_bytes,
        readableSize=readable_size,
        fileType=ext,
        truncated=truncated,
        category=category,
        summary=" ".join(summaries),
        keyFacts=list(set(keyFacts)),
        legalIssues=list(set(legalIssues)),
        credibilityConcerns=" ".join(c for c in credibilityNotes if c),
        recommendation=" ".join(r for r in recommendations if r),
        verificationNotes="\n".join(v for v in verificationNotes if v),
    )
